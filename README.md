# CogniSync â€“ Real-Time Social Intelligence Agent

> ðŸ† Built for the **Vision Possible: Agent Protocol** hackathon  
> A multi-modal AI agent that watches live video, listens to audio, understands behavior, and delivers real-time strategic advice.

---

## ðŸ—‚ï¸ Project Structure

```
visionagent/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py           # FastAPI server + /analyze endpoint
â”‚   â”œâ”€â”€ analyzer.py       # MediaPipe face/pose + audio analysis
â”‚   â”œâ”€â”€ reasoning.py      # OpenAI / Gemini LLM advice engine
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env.example
â””â”€â”€ frontend/
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ App.jsx                      # Main dashboard
    â”‚   â”œâ”€â”€ main.jsx
    â”‚   â”œâ”€â”€ index.css                    # Cyberpunk styling
    â”‚   â”œâ”€â”€ hooks/useAnalysis.js         # Webcam + audio capture hook
    â”‚   â””â”€â”€ components/
    â”‚       â”œâ”€â”€ VideoStream.jsx          # Live camera feed
    â”‚       â”œâ”€â”€ MetricsPanel.jsx         # Engagement/Stress/Confidence gauges
    â”‚       â”œâ”€â”€ SignalPanel.jsx          # Raw signal display
    â”‚       â”œâ”€â”€ AdvicePanel.jsx          # LLM advice output
    â”‚       â”œâ”€â”€ AlertSystem.jsx          # Behavioral alerts
    â”‚       â””â”€â”€ EngagementChart.jsx      # Timeline graph
    â”œâ”€â”€ package.json
    â”œâ”€â”€ vite.config.js
    â””â”€â”€ tailwind.config.js
```

---

## âš™ï¸ Environment Variables

Create `backend/.env` from the example:

```bash
cp backend/.env.example backend/.env
```

Then fill in **one** LLM key:

| Variable | Description |
|---|---|
| `OPENAI_API_KEY` | OpenAI API key (GPT-4o-mini) |
| `GEMINI_API_KEY` | Google Gemini API key (1.5 Flash) |

> If neither key is provided, a rule-based fallback generates advice automatically.

---

## ðŸš€ Installation & Run

### Backend (Python 3.10+)

```bash
cd backend

# Create virtual environment
python -m venv venv
venv\Scripts\activate        # Windows
# source venv/bin/activate   # macOS/Linux

# Install dependencies
pip install -r requirements.txt

# Copy and configure environment
copy .env.example .env
# Edit .env and add your API key

# Start backend
python main.py
# â†’ Running on http://localhost:8000
```

> **Optional:** For better emotion detection, uncomment `deepface` and `tf-keras` in `requirements.txt` and reinstall.

### Frontend (Node.js 18+)

```bash
cd frontend

# Install dependencies
npm install

# Start dev server
npm run dev
# â†’ Running on http://localhost:5173
```

Open **http://localhost:5173** in your browser.

---

## ðŸŽ¯ How It Works

```
Webcam â”€â”€â–º Frame (JPEG/base64) â”€â”€â–º POST /analyze
Mic â”€â”€â”€â”€â–º Audio features        â”€â”€â–º (same request)
                                        â”‚
                                        â–¼
                   MediaPipe FaceMesh â†’ emotion, attention
                   MediaPipe Pose     â†’ posture
                   Sharpness metric   â†’ movement
                   Audio RMS/ZCR      â†’ speech speed, tone
                                        â”‚
                                        â–¼
                   Score Engine â†’ engagement%, stress%, confidence%
                   Memory       â†’ last 5 states (short-term history)
                   Alert Engine â†’ behavioral event detection
                   LLM (GPT/Gemini) â†’ 1â€“2 line strategic advice
                                        â”‚
                                        â–¼
                   Frontend UI updates every ~1 second
```

---

## ðŸ”Œ API Reference

### `POST /analyze`

**Request:**
```json
{
  "frame_base64": "<JPEG base64 string>",
  "audio_features": {
    "speech_speed": "normal|fast|slow|silent",
    "pauses": "minimal|frequent|none",
    "tone_indicator": "neutral|calm|stressed|excited"
  }
}
```

**Response:**
```json
{
  "emotion": "happy",
  "posture": "upright",
  "attention": "high",
  "engagement_score": 82.5,
  "stress_score": 15.0,
  "confidence_score": 75.0,
  "movement": "still",
  "advice": "Strong engagement detected. This is the right moment to deepen the key message.",
  "alerts": [],
  "processing_time_ms": 340.2
}
```

### `GET /health`
Returns `{"status": "ok", "version": "1.0.0"}`.

---

## ðŸ§  AI Features

| Feature | Implementation |
|---|---|
| Emotion detection | DeepFace (if installed) â†’ MediaPipe landmark heuristic fallback |
| Head orientation / attention | MediaPipe FaceMesh yaw/pitch estimation |
| Body posture | MediaPipe Pose shoulder-hip-nose geometry |
| Movement | Laplacian sharpness variance as motion proxy |
| Audio speech rate | Zero-crossing rate (ZCR) of microphone signal |
| Audio tone | RMS amplitude + ZCR classification |
| Score fusion | Weighted multi-modal signal combination |
| Agent reasoning | GPT-4o-mini / Gemini 1.5 Flash with behavioral context prompt |
| Short-term memory | Last 5 analysis states for trend detection |
| Event detection | Engagement drop / stress spike / attention loss alerts |

---

## âœ… Performance

- Frame capture: every **1 second** (configurable)
- API response target: **< 2 seconds** end-to-end
- LLM calls: **GPT-4o-mini** (fastest OpenAI model) or **Gemini 1.5 Flash**
- Frame compression: **JPEG @ 70% quality** for minimal payload

---

## ðŸ› ï¸ Tech Stack

| Layer | Technology |
|---|---|
| Frontend | React 18 + Vite + Tailwind CSS |
| Video capture | WebRTC (`getUserMedia`) |
| Charts | Recharts |
| Backend | FastAPI + Uvicorn |
| Computer Vision | MediaPipe (FaceMesh + Pose) |
| Emotion | DeepFace (optional) / MediaPipe heuristic |
| Audio | Web Audio API (browser-side) |
| LLM | OpenAI GPT-4o-mini / Google Gemini 1.5 Flash |
